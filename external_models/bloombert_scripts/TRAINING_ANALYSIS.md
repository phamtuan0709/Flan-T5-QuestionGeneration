# ğŸ“Š PhÃ¢n TÃ­ch Training BloomBERT

## ğŸ¯ Káº¿t Quáº£ Tá»•ng Quan

### ThÃ´ng Tin Training
- **Tá»•ng epochs**: 29/50 (early stopping triggered)
- **Best validation accuracy**: **90.12%** (epoch 22)
- **Final train accuracy**: 99.64%
- **Final validation accuracy**: 89.80%
- **Overfitting gap**: ~10% (99.64% - 89.80%)

### Dataset
- Train samples: 4,940
- Validation samples: 1,235
- Batch size: 128
- Test/Val split: 20%

### Cáº¥u HÃ¬nh Optimization
- âœ… Learning rate: 1e-5 vá»›i OneCycleLR scheduler
- âœ… Weight decay: 0.01 (L2 regularization)
- âœ… Warmup: 10% of total steps
- âœ… Data shuffling: Enabled
- âœ… Class weights: Enabled (imbalanced dataset)
- âœ… Early stopping: patience=7, min_delta=0.001

---

## ğŸ“ˆ Diá»…n Biáº¿n Training

### Giai Äoáº¡n 1: Warmup & Fast Learning (Epochs 1-5)
```
Epoch 1:  Train 17.94% â†’ Val 24.05%  (LR: 1.33e-6)
Epoch 2:  Train 21.86% â†’ Val 32.55%  (LR: 3.75e-6)
Epoch 3:  Train 33.48% â†’ Val 48.02%  (LR: 6.73e-6)
Epoch 4:  Train 57.45% â†’ Val 73.28%  (LR: 9.12e-6)
Epoch 5:  Train 79.90% â†’ Val 81.78%  (LR: 1.00e-5) â† Peak LR
```
**Nháº­n xÃ©t**: Learning rate tÄƒng dáº§n (warmup), model há»c ráº¥t nhanh (+50% val acc in 5 epochs)

### Giai Äoáº¡n 2: Peak Performance (Epochs 6-15)
```
Epoch 6:  Train 86.50% â†’ Val 83.89%  (LR: 9.99e-6)
Epoch 7:  Train 89.55% â†’ Val 86.15%  (LR: 9.95e-6)
Epoch 8:  Train 91.48% â†’ Val 86.48%  (LR: 9.89e-6)
Epoch 9:  Train 93.00% â†’ Val 87.13%  (LR: 9.80e-6)
Epoch 10: Train 94.29% â†’ Val 89.07%  (LR: 9.70e-6)
Epoch 11: Train 95.14% â†’ Val 88.91%  (LR: 9.56e-6) âš 
Epoch 12: Train 95.93% â†’ Val 87.69%  (LR: 9.41e-6) âš âš 
Epoch 13: Train 96.48% â†’ Val 87.77%  (LR: 9.24e-6) âš âš âš 
Epoch 14: Train 96.82% â†’ Val 89.39%  (LR: 9.04e-6) âœ“
Epoch 15: Train 97.63% â†’ Val 89.96%  (LR: 8.82e-6) âœ“
```
**Nháº­n xÃ©t**: Val acc Ä‘áº¡t ~89-90%, báº¯t Ä‘áº§u cÃ³ dáº¥u hiá»‡u plateau

### Giai Äoáº¡n 3: Overfitting & Best Model (Epochs 16-22)
```
Epoch 16: Train 97.96% â†’ Val 89.23%  (LR: 8.59e-6) âš 
Epoch 17: Train 98.40% â†’ Val 89.72%  (LR: 8.34e-6) âš âš 
Epoch 18: Train 98.62% â†’ Val 90.04%  (LR: 8.07e-6) âš âš âš 
Epoch 19: Train 98.83% â†’ Val 89.47%  (LR: 7.79e-6) âš âš âš âš 
Epoch 20: Train 99.03% â†’ Val 89.47%  (LR: 7.49e-6) âš âš âš âš âš 
Epoch 21: Train 99.35% â†’ Val 89.39%  (LR: 7.18e-6) âš âš âš âš âš âš 
Epoch 22: Train 99.31% â†’ Val 90.12%  (LR: 6.86e-6) âœ“âœ“âœ“ BEST!
```
**Nháº­n xÃ©t**: Train acc Ä‘áº¡t 99%+, val acc dao Ä‘á»™ng 89-90%, gap tÄƒng lÃªn ~9-10%

### Giai Äoáº¡n 4: Decline & Early Stop (Epochs 23-29)
```
Epoch 23: Train 99.37% â†’ Val 89.64%  (LR: 6.54e-6) âš 
Epoch 24: Train 99.37% â†’ Val 90.04%  (LR: 6.20e-6) âš âš 
Epoch 25: Train 99.53% â†’ Val 89.55%  (LR: 5.86e-6) âš âš âš 
Epoch 26: Train 99.66% â†’ Val 89.23%  (LR: 5.51e-6) âš âš âš âš 
Epoch 27: Train 99.70% â†’ Val 89.64%  (LR: 5.17e-6) âš âš âš âš âš 
Epoch 28: Train 99.74% â†’ Val 89.72%  (LR: 4.82e-6) âš âš âš âš âš âš 
Epoch 29: Train 99.64% â†’ Val 89.80%  (LR: 4.47e-6) âš âš âš âš âš âš âš  STOP!
```
**Nháº­n xÃ©t**: Val acc khÃ´ng cáº£i thiá»‡n trong 7 epochs, early stopping kÃ­ch hoáº¡t

---

## ğŸ” PhÃ¢n TÃ­ch Váº¥n Äá»

### âœ… Nhá»¯ng Äiá»ƒm Tá»‘t
1. **Learning curve tá»‘t**: KhÃ´ng bá»‹ underfitting, model há»c Ä‘Æ°á»£c pattern
2. **Data shuffling hoáº¡t Ä‘á»™ng**: KhÃ´ng cÃ²n bá»‹ memorize thá»© tá»± nhÆ° láº§n trÆ°á»›c
3. **OneCycleLR scheduler hiá»‡u quáº£**: LR giáº£m dáº§n sau peak, giÃºp fine-tune
4. **Weight decay giÃºp giáº£m overfitting**: Tá»« 98% train / 89% val â†’ 99.6% / 90%
5. **Early stopping hoáº¡t Ä‘á»™ng Ä‘Ãºng**: Dá»«ng khi val khÃ´ng cáº£i thiá»‡n

### âš ï¸ Váº¥n Äá» ChÃ­nh: Overfitting Váº«n CÃ²n

**Biá»ƒu hiá»‡n**:
- Train accuracy: 99.64%
- Val accuracy: 89.80%
- Gap: ~10% (lÃ½ tÆ°á»Ÿng < 5%)

**NguyÃªn nhÃ¢n**:
1. **Dataset nhá»**: Chá»‰ 4,940 samples training
2. **Model capacity lá»›n**: DistilBERT (66M params) vá»›i 6 classes
3. **Imbalanced dataset**: 
   - Class 1: 2,348 samples (nhiá»u nháº¥t)
   - Class 5: 430 samples (Ã­t nháº¥t)
   - Ratio: 5.5:1
4. **Learning rate cÃ³ thá»ƒ cÃ²n cao**: 1e-5 max_lr
5. **Weight decay cÃ³ thá»ƒ chÆ°a Ä‘á»§**: 0.01

---

## ğŸ’¡ PhÆ°Æ¡ng PhÃ¡p Cáº£i Thiá»‡n

### 1ï¸âƒ£ Data Augmentation (Æ¯u tiÃªn cao)
```bash
# Sá»­ dá»¥ng nlpaug Ä‘á»ƒ augment data
python train_bloombert.py \
  --epochs 50 \
  --batch_size 128 \
  --augment  # Enable augmentation
```

**Ká»¹ thuáº­t**:
- Synonym replacement (thay tá»« Ä‘á»“ng nghÄ©a)
- Random insertion
- Random swap
- Back translation

**Æ¯á»›c tÃ­nh**: TÄƒng dataset lÃªn ~10-15K samples â†’ giáº£m overfitting ~5%

### 2ï¸âƒ£ TÄƒng Regularization
```bash
# Option A: TÄƒng weight decay
python train_bloombert.py --weight_decay 0.05  # tá»« 0.01 â†’ 0.05
```

Hoáº·c sá»­a trong `bloombert_train_patch.py`:
```python
# ThÃªm dropout cao hÆ¡n
model = BloomBERT(output_dim=6, dropout=0.3)  # tá»« 0.1 â†’ 0.3

# Label smoothing
criterion = nn.CrossEntropyLoss(
    weight=class_weights,
    label_smoothing=0.1  # ThÃªm dÃ²ng nÃ y
)
```

**Æ¯á»›c tÃ­nh**: Giáº£m train acc xuá»‘ng 97-98%, tÄƒng val acc lÃªn 90-91%

### 3ï¸âƒ£ Giáº£m Learning Rate
```bash
# Giáº£m max_lr tá»« 1e-5 â†’ 5e-6
python train_bloombert.py --lr 5e-6
```

Hoáº·c sá»­a trong code:
```python
scheduler = OneCycleLR(
    optimizer,
    max_lr=5e-6,  # Thay vÃ¬ 1e-5
    total_steps=total_steps,
    pct_start=0.15,  # TÄƒng warmup tá»« 10% â†’ 15%
    anneal_strategy='cos'
)
```

**Æ¯á»›c tÃ­nh**: Training cháº­m hÆ¡n nhÆ°ng generalize tá»‘t hÆ¡n

### 4ï¸âƒ£ Ensemble hoáº·c Mixup
```python
# ThÃªm Mixup trong training loop
def mixup_data(x, y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam
```

**Æ¯á»›c tÃ­nh**: TÄƒng val acc ~1-2%

### 5ï¸âƒ£ Balanced Sampling
```python
from torch.utils.data import WeightedRandomSampler

# TÃ­nh sample weights
class_counts = train['Label'].value_counts().sort_index()
weights = 1.0 / class_counts
sample_weights = [weights[label] for label in train['Label']]

# Táº¡o sampler
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(train),
    replacement=True
)

# Sá»­ dá»¥ng trong DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=config["batch_size"],
    sampler=sampler  # Thay vÃ¬ shuffle=True
)
```

**Æ¯á»›c tÃ­nh**: Cáº£i thiá»‡n val acc cho minority classes

---

## ğŸ¯ Khuyáº¿n Nghá»‹ Thá»±c Hiá»‡n

### Plan A: Quick Win (Thá»i gian: ~30 phÃºt)
1. Enable data augmentation: `--augment`
2. TÄƒng weight decay: 0.01 â†’ 0.05
3. Giáº£m learning rate: 1e-5 â†’ 5e-6

**Ká»³ vá»ng**: Val acc ~91-92%

### Plan B: Comprehensive (Thá»i gian: ~2 giá»)
1. Plan A +
2. ThÃªm label smoothing (0.1)
3. TÄƒng dropout (0.3)
4. ThÃªm balanced sampling
5. TÄƒng epochs lÃªn 100 (vá»›i early stopping patience=10)

**Ká»³ vá»ng**: Val acc ~92-93%

### Plan C: Research-level (Thá»i gian: ~1 ngÃ y)
1. Plan B +
2. Implement Mixup
3. Try different architectures (BERT-base, RoBERTa)
4. Hyperparameter tuning (Optuna)
5. K-fold cross validation

**Ká»³ vá»ng**: Val acc ~93-95%

---

## ğŸ“ Vá»‹ TrÃ­ Model

```
/Users/phammtuan/Study/NCKH/Flan-T5/external_models/BloomBERT/model/bloombert_model.pt
Size: 254 MB
Best Val Acc: 90.12% (epoch 22)
```

### Sá»­ Dá»¥ng Model

#### 1. Testing vá»›i test set
```bash
cd /Users/phammtuan/Study/NCKH/Flan-T5/external_models/bloombert_scripts
python test_bloombert_classifier.py
```

#### 2. TÃ­ch há»£p vÃ o Bloom-QG system
```python
# Trong bloom_qg/models/bloom_encoder.py
model = BloomBERT(output_dim=6)
model.load_state_dict(torch.load(
    "/Users/phammtuan/Study/NCKH/Flan-T5/external_models/BloomBERT/model/bloombert_model.pt"
))
model.eval()
```

#### 3. Inference trá»±c tiáº¿p
```python
from external_models.BloomBERT.src.BloomBERT import BloomBERT
import torch

model = BloomBERT(output_dim=6)
model.load_state_dict(torch.load("path/to/bloombert_model.pt"))
model.eval()

# Predict
text = "What is the capital of France?"
bloom_level = model.predict(text)  # 0-5
```

---

## ğŸš€ Lá»‡nh Cháº¡y Tiáº¿p

### Quick retry vá»›i augmentation
```bash
cd /Users/phammtuan/Study/NCKH/Flan-T5/external_models/bloombert_scripts

# Backup model hiá»‡n táº¡i
cp /Users/phammtuan/Study/NCKH/Flan-T5/external_models/BloomBERT/model/bloombert_model.pt \
   /Users/phammtuan/Study/NCKH/Flan-T5/external_models/BloomBERT/model/bloombert_model_v1_90.12.pt

# Train vá»›i augmentation
nohup python train_bloombert.py \
  --epochs 50 \
  --batch_size 128 \
  --lr 5e-6 \
  --patience 10 \
  --augment \
  > training_v2.log 2>&1 &

# Monitor
tail -f training_v2.log
```

---

## ğŸ“ Ghi ChÃº

- **So sÃ¡nh vá»›i láº§n trÆ°á»›c**: Val acc tÄƒng tá»« 89% (plateau) â†’ 90.12% (nhá» shuffling + weight decay)
- **Training time**: ~18 phÃºt cho 29 epochs (~37s/epoch)
- **Hardware**: Mac M4 vá»›i MPS (Apple Silicon)
- **Memory usage**: Stable, no OOM issues
- **Improvement rate**: ~0.3-0.5% val acc per improvement
